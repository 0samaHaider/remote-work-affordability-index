{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d044da5b-17a5-46fe-970a-1d781625fc91",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1090e96-9f35-44a6-b2bd-f476041e6f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import unidecode\n",
    "import re\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3ff81c-ae82-446e-9304-b2734d593c9f",
   "metadata": {},
   "source": [
    "### Configuration & City Lists (Pollution Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e89175dc-299d-494c-94e3-5106291f1855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of significant European cities (aligned with source naming)\n",
    "SIGNIFICANT_EUROPEAN_CITIES = [\n",
    "    'Lisbon', 'Barcelona', 'Budapest', 'Istanbul', 'Bucharest', 'Madrid', 'Sofia',\n",
    "    'Krakow (Cracow)', 'Belgrade', 'Prague', 'Porto', 'Valencia', 'Kiev (Kyiv)', 'Moscow', 'Berlin',\n",
    "    'Vienna', 'Malaga', 'Seville (Sevilla)', 'Rome', 'Faro', 'Athens', 'Warsaw', 'Minsk',\n",
    "    'Paris', 'Ljubljana', 'Florence', 'Liverpool', 'Tallinn', 'Zagreb', 'Hamburg',\n",
    "    'Naples', 'Milan', 'Split', 'Brussels', 'Dublin', 'Riga', 'Lyon',\n",
    "    'Palma de Mallorca', 'Vilnius', 'London', 'Stockholm', 'Munich', 'Marseille',\n",
    "    'Cologne', 'Amsterdam', 'Hvar', 'Dusseldorf', 'Helsinki', 'Bordeaux',\n",
    "    'Frankfurt', 'Stuttgart', 'Hanover', 'Copenhagen', 'Dresden', 'Manchester',\n",
    "    'Rotterdam', 'Saint Petersburg', 'Edinburgh', 'Dubrovnik', 'Oslo', 'Glasgow',\n",
    "    'Belfast', 'Salzburg', 'Zurich', 'Geneva', 'Valletta', 'Reykjavik'\n",
    "]\n",
    "\n",
    "# Cities excluded due to inconsistent or missing data\n",
    "CITIES_TO_EXCLUDE = ['Bordeaux', 'Faro', 'Hvar']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee076780-e241-46b7-8db3-8e0ff0bf5140",
   "metadata": {},
   "source": [
    "### Nubmeo URLs (Pollution Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d67be9bd-669d-4c26-9f06-ed3e4f97e294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numbeo URLs for Pollution\n",
    "URLS_POLLUTION = {\n",
    "    '2025_Current': 'https://www.numbeo.com/pollution/rankings_current.jsp',\n",
    "    '2023_Integration': 'https://www.numbeo.com/pollution/rankings.jsp?title=2023'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363635ce-070a-4315-9fc9-7fbc48961f95",
   "metadata": {},
   "source": [
    "### Scraping Function (Pollution Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28985658-27c1-465f-b867-251bedc93b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_numbeo_pollution(url, year_tag):\n",
    "    \"\"\"\n",
    "    Scrapes the Numbeo Pollution Index table and cleans the data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers_ua = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "        response = requests.get(url, headers=headers_ua)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        table = soup.find(\"table\", {\"id\": \"t2\"})\n",
    "        if not table:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        headers = [\"Rank\", \"City\", \"Pollution_Index\", \"Pollution_Exp_Index\"]\n",
    "        \n",
    "        rows_data = []\n",
    "        for tr in table.find_all(\"tr\")[1:]:\n",
    "            cells = tr.find_all(\"td\")\n",
    "            if len(cells) >= 4:\n",
    "                row = [cell.get_text(strip=True) for cell in cells]\n",
    "                rows_data.append(row[:4])\n",
    "        \n",
    "        df = pd.DataFrame(rows_data, columns=headers)\n",
    "        df['data_source'] = year_tag\n",
    "        \n",
    "        # Convert numeric columns\n",
    "        df['Pollution_Index'] = pd.to_numeric(df['Pollution_Index'], errors='coerce')\n",
    "        df['Pollution_Exp_Index'] = pd.to_numeric(df['Pollution_Exp_Index'], errors='coerce')\n",
    "        \n",
    "        # Clean City and Country\n",
    "        df['Country'] = df['City'].str.split(',').str[-1].str.strip().apply(unidecode.unidecode)\n",
    "        df['City'] = df['City'].str.split(',').str[0].str.strip().apply(unidecode.unidecode)\n",
    "        df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "        \n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {year_tag}: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abb3429-02cf-4e2e-b712-cfe7af1d96f1",
   "metadata": {},
   "source": [
    "### Main Execution (Pollution Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc21907e-9fe6-42ba-bf8c-93333d973cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting initial scraping...\n",
      "Initial scraping complete. Found 56 cities.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting initial scraping...\")\n",
    "df_list = [scrape_numbeo_pollution(url, tag) for tag, url in URLS_POLLUTION.items()]\n",
    "df_list = [df for df in df_list if not df.empty]\n",
    "\n",
    "if df_list:\n",
    "    df_combined = pd.concat(df_list, ignore_index=True)\n",
    "    df_combined['sort_helper'] = df_combined['data_source'].apply(lambda x: 0 if '2025' in x else 1)\n",
    "    df_combined = df_combined.sort_values('sort_helper')\n",
    "    df_unique = df_combined.drop_duplicates(subset=['city'], keep='first').copy()\n",
    "    \n",
    "    # Filtering\n",
    "    df_unique = df_unique[~df_unique['city'].isin(CITIES_TO_EXCLUDE)]\n",
    "    REFINED_CITIES = [city for city in SIGNIFICANT_EUROPEAN_CITIES if city not in CITIES_TO_EXCLUDE]\n",
    "    df_final = df_unique[df_unique['city'].isin(REFINED_CITIES)].copy()\n",
    "    \n",
    "    # Column selection\n",
    "    cols_to_keep = ['city', 'country', 'pollution_index', 'pollution_exp_index', 'data_source']\n",
    "    df_final = df_final[cols_to_keep]\n",
    "    print(f\"Initial scraping complete. Found {len(df_final)} cities.\")\n",
    "else:\n",
    "    print(\"No data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e7359-6d6c-448f-b75b-ced06b96da0a",
   "metadata": {},
   "source": [
    "### Identify Missing Cities (Pollution Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "614e4f8c-4b69-4afc-a881-75cc958e8d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cities not found:\n",
      "‚ùå Dresden\n",
      "‚ùå Dubrovnik\n",
      "‚ùå Hanover\n",
      "‚ùå Malaga\n",
      "‚ùå Palma de Mallorca\n",
      "‚ùå Salzburg\n",
      "‚ùå Seville (Sevilla)\n",
      "‚ùå Valletta\n"
     ]
    }
   ],
   "source": [
    "# --- Identify missing cities ---\n",
    "cities_we_wanted = {city for city in SIGNIFICANT_EUROPEAN_CITIES if city not in CITIES_TO_EXCLUDE}\n",
    "cities_we_got = set(df_final['city'].unique())\n",
    "missing_cities = cities_we_wanted - cities_we_got\n",
    "\n",
    "if missing_cities:\n",
    "    print(\"\\nCities not found:\")\n",
    "    for city in sorted(missing_cities):\n",
    "        print(f\"‚ùå {city}\")\n",
    "else:\n",
    "    print(\"‚úÖ No missing city!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a07cbc-cf37-4bd9-8315-b70b6c81bbb0",
   "metadata": {},
   "source": [
    "### Smart Recovery for Missing Cities (Pollution Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1176a551-35ff-4bf9-a848-5ad9389069e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recovering 8 missing cities...\n",
      "‚úÖ FOUND (Pollution: 33.86)\n",
      "‚úÖ FOUND (Pollution: 40.54)illa)... \n",
      "‚úÖ FOUND (Pollution: 26.17)\n",
      "‚úÖ FOUND (Pollution: 21.47) \n",
      "‚úÖ FOUND (Pollution: 74.23)\n",
      "‚úÖ FOUND (Pollution: 25.56)\n",
      "‚úÖ FOUND (Pollution: 22.35)\n",
      "‚úÖ FOUND (Pollution: 39.01)lorca... \n",
      "\n",
      "‚úÖ Recovered 8 cities added to df_final\n"
     ]
    }
   ],
   "source": [
    "def scrape_pollution_smart_fallback(city_name):\n",
    "    \"\"\"\n",
    "    Attempts multiple name variants to extract pollution metrics from Numbeo.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    # Variant A: Name before parentheses\n",
    "    var_a = city_name.split('(')[0].strip().replace(\" \", \"-\").title()\n",
    "    candidates.append(var_a)\n",
    "    \n",
    "    # Variant B: Name inside parentheses\n",
    "    match = re.search(r'\\((.*?)\\)', city_name)\n",
    "    if match:\n",
    "        var_b = match.group(1).strip().replace(\" \", \"-\").title()\n",
    "        candidates.append(var_b)\n",
    "        \n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "    for candidate in candidates:\n",
    "        url = f\"https://www.numbeo.com/pollution/in/{candidate}\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                \n",
    "                values = {'pollution_index': None, 'pollution_exp_index': None}\n",
    "                label_map = {\n",
    "                    'pollution_index': \"Pollution Index:\",\n",
    "                    'pollution_exp_index': \"Pollution Exp. Index:\"\n",
    "                }\n",
    "                \n",
    "                found_something = False\n",
    "                for col_name, search_text in label_map.items():\n",
    "                    label_el = soup.find(string=re.compile(re.escape(search_text)))\n",
    "                    if label_el:\n",
    "                        parent = label_el.parent\n",
    "                        value_container = parent.find_next(\"td\")\n",
    "                        if value_container:\n",
    "                            raw_text = value_container.get_text(strip=True)\n",
    "                            match_num = re.search(r\"(\\d+\\.\\d+)\", raw_text)\n",
    "                            if match_num:\n",
    "                                values[col_name] = float(match_num.group(1))\n",
    "                                found_something = True\n",
    "                \n",
    "                if found_something:\n",
    "                    return values\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "# Mapping for missing cities to correct countries\n",
    "CITY_TO_COUNTRY = {\n",
    "    \"Malaga\": \"Spain\",\n",
    "    \"Seville (Sevilla)\": \"Spain\",\n",
    "    \"Hanover\": \"Germany\",\n",
    "    \"Dubrovnik\": \"Croatia\",\n",
    "    \"Valletta\": \"Malta\",\n",
    "    \"Salzburg\": \"Austria\",\n",
    "    \"Dresden\": \"Germany\",\n",
    "    \"Palma de Mallorca\": \"Spain\"\n",
    "}\n",
    "\n",
    "# --- Recover missing cities ---\n",
    "if missing_cities:\n",
    "    print(f\"\\nRecovering {len(missing_cities)} missing cities...\")\n",
    "    new_rows = []\n",
    "\n",
    "    for city in missing_cities:\n",
    "        print(f\"üîç Searching: {city}...\", end=\" \")\n",
    "        time.sleep(1)\n",
    "        val_dict = scrape_pollution_smart_fallback(city)\n",
    "\n",
    "        if val_dict:\n",
    "            print(f\"‚úÖ FOUND (Pollution: {val_dict.get('pollution_index')})\")\n",
    "            row = {\n",
    "                'city': city,\n",
    "                'country': CITY_TO_COUNTRY.get(city, None),\n",
    "                'data_source': 'Single_Page_Recovery'\n",
    "            }\n",
    "            row.update(val_dict)\n",
    "            new_rows.append(row)\n",
    "        else:\n",
    "            print(\"‚ùå Not found.\")\n",
    "\n",
    "    if new_rows:\n",
    "        df_new = pd.DataFrame(new_rows)\n",
    "\n",
    "        # Ensure numeric columns are floats\n",
    "        for col in ['pollution_index', 'pollution_exp_index']:\n",
    "            if col in df_new.columns:\n",
    "                df_new[col] = df_new[col].astype(float)\n",
    "\n",
    "        # Add recovered rows to the main DataFrame\n",
    "        df_final = pd.concat([df_final, df_new], ignore_index=True)\n",
    "        print(f\"\\n‚úÖ Recovered {len(new_rows)} cities added to df_final\")\n",
    "else:\n",
    "    print(\"‚úÖ No missing cities to recover.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae30e56-0b98-43f1-bd99-ec576eeab145",
   "metadata": {},
   "source": [
    "### Final Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42f8b737-0d01-4675-8eac-d5af02523d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FINAL VERIFICATION ---\n",
      "Total cities in DataFrame: 64\n",
      "\n",
      "--- Recovered cities details ---\n",
      "                         city  pollution_index  pollution_exp_index  \\\n",
      "city_index                                                            \n",
      "12                    Dresden            22.35                  NaN   \n",
      "14                  Dubrovnik            21.47                  NaN   \n",
      "22                    Hanover            26.17                  NaN   \n",
      "33                     Malaga            33.86                  NaN   \n",
      "42          Palma de Mallorca            39.01                  NaN   \n",
      "51                   Salzburg            25.56                  NaN   \n",
      "52          Seville (Sevilla)            40.54                  NaN   \n",
      "59                   Valletta            74.23                  NaN   \n",
      "\n",
      "                     data_source  \n",
      "city_index                        \n",
      "12          Single_Page_Recovery  \n",
      "14          Single_Page_Recovery  \n",
      "22          Single_Page_Recovery  \n",
      "33          Single_Page_Recovery  \n",
      "42          Single_Page_Recovery  \n",
      "51          Single_Page_Recovery  \n",
      "52          Single_Page_Recovery  \n",
      "59          Single_Page_Recovery  \n",
      "\n",
      "‚úÖ No duplicate cities found.\n"
     ]
    }
   ],
   "source": [
    "# Sort alphabetically and reset index\n",
    "df_final = df_final.sort_values(by='city').reset_index(drop=True)\n",
    "df_final.index = df_final.index + 1\n",
    "df_final.index.name = 'city_index'\n",
    "\n",
    "# Display specific recovery check\n",
    "recovered_cities = df_final[df_final['data_source'] == 'Single_Page_Recovery']\n",
    "\n",
    "print(\"\\n--- FINAL VERIFICATION ---\")\n",
    "print(f\"Total cities in DataFrame: {len(df_final)}\")\n",
    "print(\"\\n--- Recovered cities details ---\")\n",
    "if not recovered_cities.empty:\n",
    "    print(recovered_cities[['city', 'pollution_index', 'pollution_exp_index', 'data_source']])\n",
    "else:\n",
    "    print(\"No recovered cities found.\")\n",
    "\n",
    "# Quick check for duplicates\n",
    "duplicates = df_final[df_final.duplicated(subset=['city'])]\n",
    "if not duplicates.empty:\n",
    "    print(f\"\\n‚ö†Ô∏è WARNING: Found {len(duplicates)} duplicates!\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No duplicate cities found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862df729-ec07-4f55-9117-bffc5870add5",
   "metadata": {},
   "source": [
    "### Delete unnesessary column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24fc81e6-01a1-48e6-b517-fd7c360b847f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Column 'pollution_exp_index' dropped.\n",
      "\n",
      "--- FINAL CLEAN DATASET PREVIEW ---\n",
      "                 city         country  pollution_index   data_source\n",
      "city_index                                                          \n",
      "1           Amsterdam     Netherlands             22.6  2025_Current\n",
      "2              Athens          Greece             55.2  2025_Current\n",
      "3           Barcelona           Spain             63.0  2025_Current\n",
      "4             Belfast  United Kingdom             26.5  2025_Current\n",
      "5            Belgrade          Serbia             69.2  2025_Current\n"
     ]
    }
   ],
   "source": [
    "if 'pollution_exp_index' in df_final.columns:\n",
    "    df_final = df_final.drop(columns=['pollution_exp_index'])\n",
    "    print(\"üóëÔ∏è Column 'pollution_exp_index' dropped.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Column 'pollution_exp_index' not found.\")\n",
    "\n",
    "# View the final clean dataset\n",
    "print(\"\\n--- FINAL CLEAN DATASET PREVIEW ---\")\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be99f2e-3de1-455f-a9c1-26bf12ff0f61",
   "metadata": {},
   "source": [
    "### Export CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2637f7ff-a3d6-4911-82a3-872cb1c2f548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " File successfully saved as: significant_european_cities_pollution_index.csv\n",
      " Directory: C:\\Users\n"
     ]
    }
   ],
   "source": [
    "file_name = 'significant_european_cities_pollution_index.csv'\n",
    "\n",
    "try:\n",
    "    df_final.to_csv(file_name, index=False, encoding='utf-8')\n",
    "    print(f\" File successfully saved as: {file_name}\")\n",
    "    print(f\" Directory: {os.getcwd()}\")\n",
    "except Exception as e:\n",
    "    print(f\" An error occurred while saving: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
